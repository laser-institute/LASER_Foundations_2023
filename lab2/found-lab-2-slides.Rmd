---
  title: "Foundation lab 2 - LA Methods"
  author: "Jeanne McClure"
  institute: "Instituttional Affiliation"
  date: '`r format(Sys.time(), "%B %d, %Y")`'
  output:
    xaringan::moon_reader:
      css:
       - default
       - css/laser.css
       - css/laser-fonts.css
      lib_dir: libs                        # creates directory for libraries
      seal: false                          # false: custom title slide
      nature:
        highlightStyle: default         # highlighting syntax for code
        highlightLines: true               # true: enables code line highlighting
        highlightLanguage: ["r"]           # languages to highlight
        countIncrementalSlides: false      # false: disables counting of incremental slides
        ratio: "16:9"                      # 4:3 for standard size,16:9
        slideNumberFormat: |
         <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
         </div>
---
class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```
```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`


---
# Recap from Data Structures Presentation

--

- Types of Data Used in LA (Sources)

--

- Characteristics of Data (Format)

???

Before we go on lets take just a few minutes, how are you all feeling so far?
On a quick recap from our data structures presentation we discussed the different types of data found commonly in Learning Analytics. 

**Types of Data** included:

- Digital Learning Environments
  + games 
  + learning management systems
  + Intelligent tutoring systems
  + MOOCs
- Administrative Data
  + Student information Systems
  + statewide longitudinal data systems
- Sensors and Multimodal
  + Sensors
  + Speech and video analysis

**Characteristics of Data** included:
- Structured
  + Quantitative and relational
- Unstructured
  + Qualitative in nature and un-relational
- Semi-Structured
  + Quantitative and Triangular
- Meta-Data
+ Quantitative, data about data.


  
---
# Learning Analytics Workflow Agenda


.pull-left[**Part-1  Conceptual Overview**

- Methods in Learning Analytics

]

.pull-right[**Part-2  Code-Along**

- Workflow 
  + Wrangle
    - Tidy
    - join

]

.footnote[Pre-Reading:
[Learning Analytics Goes to School, (Ch. 2, pp. 41 - 34) By Andrew Krumm, Barbara Means, Marie Bienkowski](https://github.com/laser-institute/essential-readings/blob/main/foundation_labs/foundlab_1/krumm_2018.pdf)
]
---

class: clear, inverse, middle, center

Part 1:

----

Methods used and Ethical Considerations: Conceptual Overview

---

#  Methods

- Predictive Analysis

--
- Social Learning Analytics

--
- Discourse Analysis

--
- Text Analysis

--
- Multimodal Analysis




???

**Predictive Analysis** supports student learning by predicting future

- Learning analytics aims to comprehend and enhance the learning process (Macfadyen & Dawson, 2010). Consequently, it is not unexpected that initial investigations predominantly concentrated on establishing predictive frameworks for student retention and academic achievement (Gašević et al., 2015; Siemens, 2013). This emphasis on identifying or foreseeing students who may be at risk of academic failure during the early stages of their academic journey is driven by the economic advantages associated with student retention.

- Course Signals first predictive dashboard by Arnold and Pistilli, 2012
consisted of a predictive model for detecting students’ at-risk of course
failure, and a dashboard which uses a traffic light analogy to visualise
individual students’ risk of failure (i.e., green–no risk, yellow–moderate risk,
red–high risk). The predictive model underpinning the Course Signals
software is based on a wide range of variables including, LMS engagement
activity, demographics, and past academic performance

**Social Learning Analytics**

- Social Learning analytics involves understanding student interactions through social network analysis. 
Social network analysis (SNA) has emerged as a fundamental approach in learning analytics research. SNA in learning analytics involves extracting peer interactions from online forums to assess students' sense of community, examining creative capacity, exploring the relationship between learners' social centrality and learning outcomes, and visualizing and analyzing patterns in interactions from social learning activities between students and teachers, among other research objectives.


**Discourse Analysis**

- Discourse Analysis is a type of learning analytics that focuses on using textual discourse data for supporting student learning. It focuses on studying language in use, specifically the analysis of spoken or written communication in its broader context. Its primary application lies in analyzing students' online communications, including transcripts of their online discussions, chat room interactions, and communications across various social media platforms such as Twitter, Facebook, and blogs. Analysis of student communication transcripts and linguistically modelling student dialogue provides ways of capturing social aspects of student learning.

**Text Analysis**

- On the other hand, text analysis, examines written or spoken texts to identify patterns, themes, and structures within the text itself. Text analysis typically involves quantitative techniques to analyze large sets of texts, using coding and categorization schemes to extract relevant information and gain insights into the content.

**Comparing Discourse vs Textual Analysis**

- While both discourse analysis and text analysis involve analyzing language, discourse analysis emphasizes the social and contextual aspects of communication, whereas text analysis focuses more on examining the content and structure of the text itself. They can be complementary approaches, with discourse analysis providing a deeper understanding of the meaning-making processes within social contexts, while text analysis provides systematic and quantitative insights into textual data.

**Multimodal Analysis**
- an approach able to provide more specific learning models to account for alternate learning designs and teaching practices. Multimodal learning analytics employs diverse data sources to enhance the understanding of learning across different environments, encompassing both in-person and online educational settings. Multimodal learning analytics aims to gather complementary forms of learning-related data, enabling a comprehensive intricacies involved in the learning processes. This analysis tend to go beyond more traditional trace and survey data to incorporate various sensor data streams that capture gestures, gaze, or speech.
---

# Legal and Ethical Considerations

- Ethics of ‘Big Data’ and AI
--

- Problem of opaque ‘black box’ algorithms
--

- Training machine learning classifiers on biased data sets
--

- Incorrectly predicting someone’s behavior

???




---

class: clear, inverse, middle, center
Part 2:

----

Code-Along


---
# WORKFLOW: Wrangle

<img src="img/wrangle_wflow.png" height="425px"/>

???

**WRANGLE  TAB**

Wrangling or sometimes called "munging or pre-processing" entails the work of manipulating, cleaning, transforming, and merging data.
- **manipulating** involves identifying, acquiring, and importing data into analysis software.

- Wickham & Grolemund suggest cleaning data involves ensuring that each variable is in its own column, each observation is in its own row, and each value

is in its own cell within a dataset.
This is called Tidying your data and is part of the philosophy that informs the tidyvers suite.

  + Krumm et al adds that **data cleaning** also involves identifying and remediating missing data, extreme values, and ensuring consistent use of identifier, key, or linking variables.
-  transforming variables, such as recoding
categorical variables and rescaling continuous variables.   
+ These types of transformations are the initial building blocks for **exploratory data analysis**

-  One of the biggest value add ons is merging once disparate data sources.

  + For example: merging data from a student information system that stores student grades with data from a digital learning environment that stores students’ longitudinal interactions can unlock what student do and do not do on a day to day basis.
  
---
#First Prepare


```{r eval=FALSE, echo=TRUE, warning=FALSE, message=FALSE}

#load Library
library(tidyverse)

#load with read_csv package
time_spent <- read_csv("data/log-data.csv")
gradebook <- read_csv("data", "gradebook-summary.csv")
survey <- read_csv("data", "survey.csv")

```


---
# Workflow: WRANGLE

1. **Import**
2. **Tidy**
3. **Separate**
3. **Join**



---
#Tidy

.center[
<img src="img/dplyr.png" height="425px"/>
]

---
#Separate



```{r eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
time_spent %>%  
  separate(course_id,
           c("subject", "semester", "section"))


knitr::kable(head(time_spent, n=5), format = 'html')

```
---
# Mutate

```{r eval=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
# mutate minutes to hours on time spent and save as new variable.
time_spent_2 <- time_spent_2 %>% 
  mutate(time_spent_hours = time_spent / 60)
time_spent_2
```


---
#Join

.center[
<img src="img/join.png" height="425px"/>
]

---
#Janitor Package

{[Janitor](packagehttps://cran.r-project.org/web/packages/janitor/vignettes/janitor.html)}
.center[
<img src="img/janitor.png" height="425px"/>
]


???


Today we will focus on *Wrangle* phases. **Wrangle**: Wrangling data entails the work of manipulating, cleaning, transforming, and merging data. 



**PREPARE TAB**
In this part of the workflow, **Prepare**, load your libraries. If this is the first time using the library then you will need to install first using the 'install.packages("")' function before using the `library()` function. Add the required data needed for analysis.



**WRANGLE TAB**
About 45- 50%% of your time is spent in cleaning the data.

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled (Krumm et al, 2018). 

In the wrangle section we are going to:

a.  **Import Data**. In this section, we introduce the `read_csv()` function for working with CSV files and revisit some key functions for inspecting our data.

b.  **Tidy Data**. We introduce the `separate()` and `clean_names()` functions for getting our data nice and tidy, and revisit the `mutate()` for creating new variables.

c.  **Join Data**. We conclude our data wrangling by introducing  a`join()` function for merging our processed files into a single data frame for analysis.




**TIDY TAB**

There are a lot of functions in dplyr that help you to solve specific problems We will be using the seperate and mutate functions. 

1. we will load `time_spent` and run the `separate()` function with
the `course_id` variable to split up the subject, semester, and section
so we can use them later on. In other words, whereas above we separated
the variable `course_variable`, in the data set we'll use here, we'll
separate the `course_id` variable.

Once we've processed the data how we
would like, we have to assign, or save, the results back to the name for
the data with which we have been working. This is done with the
assignment operator, or the \<- symbol.

2. We'll use
`mutate()` to create a new variable for the percentage of points each
student earned; keep in mind as you work through these steps how so many
parts of wrangling data involves either changing a variable or creating
a new one. For these purposes, mutate can be very helpful. 

Let's process `time_spent` variable that is in number of *minutes* that students spent on the
course LMS. We will change it to
`time_spent_hours`, that represents the number of *hours* that students spent on the course LMS.

We will also process gradebook data and  Survey data - which we will use a new package called janitor. 

Let's look at the survey data again. We notice that the data You may noticed that `student_ID` is not formatted the same as `student_id` in our other files. This is important because in the next section when were "join," or merge, our data files, these variables will
need to have identical names.

Fortunately the
{[janitor](https://garthtarr.github.io/meatR/janitor.html)} package hassimple functions for examining and cleaning dirty data. It was builtwith beginning and intermediate R users in mind and is optimized foruser-friendliness. There is also a handy function called `clean_names()`in the {janitor} package for standardizing variable names.





**JOIN TAB**
Next, we will join the data together. There are many different join functions that can be used to join data sets.

You may already be aware that your single
analysis involves multiple data files.

While in some cases it is
possible to analyze each data set individually, it is often useful (or
necessary, depending upon your goal) to join these sources of data
together. This is especially the case for learning analytics research,
in which researchers and analysts often are interested in understanding
teaching and learning through the lens of multiple data sources,
including digital data, institutional records, and survey data, among
other sources. In all of these cases, knowing how to promptly join
together files---even files with tens of thousands of hundreds of
thousands of rows---can be empowering.

A key (pun intended) with joins is to consider what variable(s) will
serve as the *key*. This is the variable to join by.

A key must have two characteristics; it is:

-   a character string--- a word (thus, you cannot join on a number
    unless you "coerce" or change it to be a character string, first)

-   present in both of the data frames you are joining.

To join two datasets, it is important that the *key* (or *keys*) on
which you are joining the data is formatted identically. The key
represents an identifier that is present in both of the data sets you
are joining. For instance, you may have data collected from (or created
about) the same students that are from two very different sources, such
as a self-report survey of students and their teacher-assigned grades in
class.

While some of the time it takes some thought to determine what the key
is (or what the keys are---you can join on multiple keys!), you need just one variable that meets both of the above
characteristics.

We're going to use a single join function, `full_join()`. In
the code below, join `gradebook` and `time_spent`; type the names of
those two data frames as arguments to the `full_join()` function in a
similar manner as in the `full_join()` code above, and then run this
code chunk. For now, don't specify anything for the `by =` part of the
function.

You may notice a message in the console or first output box above that
says `Joining, by = c("student_id", "Course", "Subject", "Section")`.
This is telling us that these files are being joined on the basis of all
four of these variables matching in both data sets; in other words, for
rows to be joined, they must match identically on all four of these
variables.


For more on joins:
<https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti>

GREAT job on wrangling your data!

]


---

class: inverse, clear, center

## .font130[.pull-left[**What's next?**]]

<br/><br/><br/><br/><br/>

.pull-left-wide[.left[.font100[
-  Make sure to complete the R Programming primers:  [Tidy your Data](https://rstudio.cloud/learn/primers/4)

-  Complete the badge requirement document from your lab 2 folder [foudationlab2_badge - Data Sources](https://github.com/laser-institute/foundational-skills/blob/master/foundation_lab_2/foundationlab2_badge.Rmd).
]]
]

## .font175[.center[Thank you! Any questions?]]



---

<img src="img/team_2022.png" height="550px"/>